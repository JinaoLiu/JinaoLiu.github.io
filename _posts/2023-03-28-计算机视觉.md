---
layout:     post
title:      OC 内存管理的基本概念
subtitle:   OC 内存管理的基本概念以及应用
date:       2023-03-28
author:     jinaoliu
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - iOS		
    - 内存管理	
    - 操作系统
---



# 计算机视觉

## 一、计算机视觉简介

定义：用计算机模拟人类的视觉系统，去完成人类的视觉任务

基本任务：图像分类 目标检测与定位 语义分割和实例分割

图像分类：对图像进行分类

语义分割：对每个像素点进行分类且不区分相同类别的个体

分类+定位：单目标检测

目标检测：多个目标的分类与定位

实例分割：实例分割是一个与目标检测密切相关的概念。但是，与目标检测不同，输出是包含对象的掩码（或轮廓），而不是包围框。与语义分割不同的是，我们不会标记图像中的每个像素；我们只对寻找特定对象的边界感兴趣

## 二、图像分类

CV的核心任务：图像分类

计算机的挑战：语义鸿沟 尺度差异 视觉差异 类内差异

交叉验证是个不错的选择，对于大型数据集和深度学习较少使用

数据驱动的算法

KNN

优点：简单

缺点：1.不具有泛化能力 2.像素距离 $\ne$语义距离 3. 训练即需要均匀分布，否则导致curse of dimensionality

曼哈顿距离：两点之间的x和y距离

欧式距离：两点之间的距离

Linear classifier

线性分类器很难适用于的数据分布：离散象限 1<L2距离<2 孤岛数据

## 三、损失函数和优化

### 线性分类器

**Multiclass SVM loss（SVM分类器）**

<img src="../../桌面/images/image-20221217095642298.png" alt="image-20221217095642298" style="zoom:50%;" />

Q：如果有输出分数发生微小改变，如何影响损失函数？

A：$s_{j}-s_{y_{i}}<-1$时，没有影响。反之，会略微改变损失函数的值

Q：损失函数的最大值和最小值分别是多少？

A：最小值为0,最大值为正无穷

Q：假如初始化 W 接近0，导致所有输出分数都约等于0, 那么$L_{i}$约等于多少？

A: C -1，C为类别数

Q：假如去掉𝑗 ≠ 𝑦 ! 的限制，损失函数如何变化？

A：加1

Q：假如在$L_{i}$中使用求平均代替求和，模型如何变化？

A：不会影响最终的W

Q：假如在$L_{i}$中使用$max(0,s_{j}-s_{y_{i}}+2)$ 代替$max(0,s_{j}-s_{y_{i}}+1)$，有什么影响？

A：没有影响，SVMloss只关注输出分数之间的差异，这里的常数只起到scale权重的作用

Q：假如𝑾使得𝐿 = 0（完美），请问𝑾是否唯一？

A：不是，W * c 也使得L =0, c 为任意正整数

**Cross-Entropy loss（softmax分类器）**

<img src="../../桌面/images/image-20221217105623922.png" alt="image-20221217105623922" style="zoom:50%;" />

Q：如果有输出分数发生微小改变，如何影响损失函数？

A：是的，正确类别和错误类别输出分数差距越大，损失函数越小

Q：损失函数的最大值和最小值分别是多少？

A：最小值为0,最大值为正无穷

Q：假如初始化 W 接近0，导致所有输出分数都约等于0, 那么$L_{i}$约等于多少？

A: logC，C为类别数

### 正则化

将复杂模型简单化，防止模型过度拟合训练集

$\lambda R(W)$

缩小权重空间

调整权重偏好的分布

提高模型泛化能力

有$L_{1}$和$L_{2}$两种正则化，$L_{1}$偏向于使权重集中在少数输入像素上，$L_{2}$偏向于使权重分布在所有像素上

### 优化

负梯度方向是下降最快的方向

Hinge Loss和Cross Entorpy Loss梯度推导？？

min-batch：把所有样本分为 m 个 batch ，每跑一个 batch 更新一次参数，每个 batch 近似估计整体，把 m 个 batch 跑完称为一个 epoch 结束

## 四、反向传播

反向传播的计算

## 五、卷积神经网络

全连接神经网络的缺点：1.参数过多 2.对于图像没有不能展示空间特性

卷积神经网络：参数共享 局部连接

卷积参数 卷积核计算

卷积的反向传播计算

## 六、神经网络的训练

### 激活函数

|                           激活函数                           |                             优点                             |                             缺点                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="images/image-20221217213352491.png" alt="image-20221217213352491" style="zoom: 33%;" /> |       1.将数值压缩到（0, 1）之间  2.曲线平滑，便于求导       |    1.容易饱和输出 2.不是零均值 3. exp（）函数计算复杂度高    |
| <img src="images/image-20221217213417986.png" alt="image-20221217213417986" style="zoom: 33%;" /> |       1.将数值压缩到（1, -1）之间 2.曲线平滑，便于求导       |          1. 容易饱和输出 2. exp() 函数计算复杂度高           |
| <img src="images/image-20221217213447723.png" alt="image-20221217213447723" style="zoom:33%;" /> | 1.在正区间不会饱和 2.计算复杂度极低 3.收敛速度比sigmoid和tanh快 | 1.不是零均值 2.不压缩数据，数据幅度随网络深度不断增大 3.神经元坏死 |
| <img src="images/image-20221217213523204.png" alt="image-20221217213523204" style="zoom:33%;" /> | 1. 不会造成饱和 2. 计算复杂度低 3. 收敛速度比sigmoid和tanh快 4.近似零均值 5.解决ReLU的神经元坏死问题 |        1. 数值幅度不断增大 2. 实际表现不一定比ReLU好         |
| <img src="images/image-20221217214536263.png" alt="image-20221217214536263" style="zoom:33%;" /> | 1.不易造成饱和 2. 收敛速度比Sigmoid和tanh快 3. 近似零均值 4. 解决ReLU的神经元坏死问题 |         1. exp() 计算复杂度高 2. 表现不一定比ReLU好          |

### 权重参数初始化

参数初始化过小

回传梯度快速接近0, 梯度消失，靠近输入层的梯度无法更新

参数初始化过大（> 1）

回传梯度快速增大，梯度爆炸，靠近输入层的梯度更新太快

### 正则化

#### Batch Normalization

- 内部输出的分布由于参数变化而变化

- 导致激活容易饱和或者趋近0

- 神经网络训练不易收敛

优化了梯度流，使深度网络训练起来更加容易
可以使用较大learning rate，加速收敛
受权重参数初始化影响较小
在训练过程中起到正则化的作用
推理时和FC/Conv层融合，几乎不增加开销

#### Dropout

随机drop一些output

将神经元被drop的概率平均到输出权重上

近似于计算训练时该神经元的期望输出

训练时：增加随机噪声
推理时：将噪声的影响平均化

### 优化器

#### SGD

缺点：陷入局部最小值和鞍点参数很难继续更新

#### Momentum

给梯度加上速：梯度越大，速度增量越大

#### AdaGrad

梯度陡峭的方向学习率小，平缓的方向学习率大

#### Adam

动态变化：训练过程的一般设置，超参数的选择，优化器，参数更新

模型评价：模型集成，测试时补充

## 八、DL硬件和软件

CPU：核心数量少 但是主频高，适合做通用顺序任务

GPU：核心数量多，但主频低，适合做通用并行计算

自动计算梯度：

Pytorch：loss.backward()

tensorflow: tf.GradientTape().gradient()

动态图优点：代码结构灵活清晰 方便调试

静态计算图优点：

高效，不用反复构建计算图及其节点（正向和反向）计算图结构固定

编译器可以自动寻找优化可能

可以序列化，方便跨语言调用

## 十、循环神经网络

## 十一、注意力机制

## 十二、生成模型

### PixelRNN、PixelCNN

PixelRNN和PixelCNN都是生成时仍然生成一个像素点

优点：显示计算图像生成概率，生成图像直接和原始图像比较，易于优化，训练稳定

缺点：慢

改进：sigmoid代替softmax 将RGB像素视为整体 Res Connections Dropout

### VAE

优点：

1.易于优化，训练稳定

2.训练较快

缺点：

1.优化的是真实目标函数极大似然下界

### GAN

GAN不再显示定义概率密度函数

Generator尽可能使假图片的output接近1
